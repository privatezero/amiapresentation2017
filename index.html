<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Perceptual Hashing!</title>

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/black.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section data-markdown>
					<textarea data-template>
					### Automation and Similarity: An Introduction to Perceptual Hashing
					#### Andrew Weaver
					Washington State University Libraries
					</textarea>
				</section>

				<section data-markdown>
					<textarea data-template>
					#### privatezero.github.io/amiapresentation2017
					<a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/80x15.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.
					</textarea>
				</section>

				<section data-markdown>
					<textarea data-template>
					### What is perceptual hashing??
					* Creates links between materials in the way a human would perceive them
					* Kind of like fixity for content instead of file integrity

					Note: People have most likely used in some form - Shazam, Google Reverse Image Search etc
					</textarea>
				</section>
				<section data-background-image=Resources/cougcolor.jpg>
				<div class="fragment fade-in">
				<div class="contrast">
				<p>Left and right image same to viewer ; totally different to computer</p>
				<div class="citation">
				<p><a href="http://content.libraries.wsu.edu/cdm/compoundobject/collection/wsu_fb/id/4034/rec/162">Image: WSU Libraries; Manuscripts, Archives and Special Collections</a></p>
				</div>
				</div>
				</div>
				<aside class="notes">
				1964- Beatlemania!
				</aside>
				</section>

				<section data-background-image=Resources/cougcolor.png>
				<div class="fragment fade-in">
				<div class="contrast">
				<p>Previous Slide Background was .jpg, This slide is .png</p>
				<div class="citation">
				<p><a href="http://content.libraries.wsu.edu/cdm/compoundobject/collection/wsu_fb/id/4034/rec/162">Image: WSU Libraries; Manuscripts, Archives and Special Collections</a></p>
				</div>
				</div>
				</div>
				</section>

				<section data-markdown>
					<textarea data-template>
					### Why bother with perceptual hashing??
					* Easy discovery of 'B-Roll' use
					* Quickly discover duplicates and derivative files
					* Potential Patron access point
					* Automated discovery of relationships between records

					Note:
					Another form of potentially powerful future proofing! Makes content more searchable (like OCR or speech to text).
					</textarea>
				</section>

                <section data-markdown>
                    <textarea data-template>
                    #### Problem: How to make content machine readable?
                    ##### Answer: Transformations (lots of 'em!)
                    Note:
                    Will now talk about a couple examples for audio and video hashing
                    </textarea>
                </section>

				<section data-markdown>
					<textarea data-template>
					#### Audio Hashing using Chromaprint (and [pyacoustid](https://pypi.python.org/pypi/pyacoustid))

					Note:
					Audio hashing has a decent community around it - integrated into tools such as VLC for metadata tagging!
					</textarea>
				</section>

				<section data-markdown>
					<textarea data-template>
					### Chromaprint Library
					Converts audio input to a sample rate of 11025 Hz before generating representations of the content in musical notes (ignoring octave) via frequency analysis.
					
					Results subjected to a series of filters measuring contrasts between sub-areas which create the final fingerprint.

					Note:
					Also involves overlapping audio frames etc. This measuring of contrasts similar to MPEG-7 standard - will talk more later!
					</textarea>
				</section>

				<section>
					<div class="citation">
					<p>https://oxygene.sk/2011/01/how-does-chromaprint-work</p>
					</div>
					<img src="Resources/chromaprint.png" alt="Chromaprint example" width=40% height=40%>
					<aside class="notes">
					Online actoustid database reliant on length of audio as well as fingerprint for ID, so not robust to partial files. BUT open source and locally hostable - presumably changes could be made!
					</aside>
				</section>

				<section data-markdown>
					<textarea data-template>
					#### Video hashing using MPEG-7 Standard (and [FFmpeg](https://avpres.net/FFmpeg/#ch1))
					Note:
					This is a built in filter in FFmpeg now! Anyone with FFmpeg installed can now take advantage of this!
					</textarea>
				</section>

				<section data-markdown>
					<textarea data-template>
					Process down converts each frame to 32x32 8 bit images and extracts series of averages and differences of the luma content for subsections of new frames.
					
					Elements of these frame signatures are then plotted on a histogram in ninety frame chunks, which is then 'binarized' to create rough signatures.
					</textarea>
				</section>

				<section>
					<div class="citation">
					<p>https://en.wikipedia.org/wiki/Hamming_distance#/media/File:Hamming_distance_4_bit_binary.svg</p>
					</div>
					<img src="Resources/hamming.png" alt="Hamming Distance example" width=100% height=100%>
					<aside class="notes">
					Binarization of output allows comparison via 'Hamming Distance' - number of binary digits that would have to be changed to make strings identical.
					</aside>
				</section>

 				<section>
 					<div class="citation">
 					<p>Chiariglione, L. (2012). The MPEG representation of digital media. (pp. 88). New York, NY: Springer.</p>
 					</div>
					<img src="Resources/lumafingerprint.png" alt="Video hash sampling" width=60% height=60%>
					<aside class="notes">
					Example showing region of theoretical 32X32 pixel frame used for average luma value (left) and subregions used for calculating 'difference values' right
					</aside>
				</section>

				<section data-background-iframe="https://archive.org/details/SMA_524" data-background-interactive>
					<div class="contrast">
						<pre><h3>Video Source</h3></pre>
					</div>
				</section>

				<section data-background-video="Resources/ballard.ogv" data-background-video-loop data-background-video-muted>
					<pre><code data-trim data-noescape>
					ffmpeg -i https://archive.org/download/SMA_524/SMA_5153.mp4 \
					-i 'https://privatezero.github.io/amiapresentation2017/Resources/ballard.ogv' \
					-filter_complex signature=detectmode=full:nb_inputs=2 -f null -
					</code></pre>
					<aside class="notes">
					Filter allows either direct analysis of inputs, or generation of a fingerprint file.
					</aside>
				</section>

				<section>
					<code>[Parsed_signature_0 @ 0x126cc00] matching of video 0 at 581.672567 and 1 at 1.668333, 295 frames matching
[Parsed_signature_0 @ 0x126cc00] whole video matching</code>
					<aside class="notes">
					Right around 9:40
					</aside>
				</section>

				<section data-markdown>
					<textarea data-template>
					#### Experimental application at CUNY-TV
					</textarea>
				</section>

				<section data-markdown>
					<textarea data-template>
					* Create Fingerprints
					
					[Bash script](https://github.com/mediamicroservices/mm/blob/master/makefingerprint) controlling FFmpeg

					* Store Fingerprints
					
					[Bash functions](https://github.com/mediamicroservices/mm/blob/master/mmfunctions#L108) parsing XML output of fingerprints to DB/Adding to AIPs

					* Query Fingerprints
					
					[Bash script](https://github.com/mediamicroservices/mm/blob/master/searchfingerprint) controlling FFmpeg and FFplay to generate/compare fingerprint/time stamp against stored fingerprints and display video of matches.

					Note:
					Added 'makefingerprint' microservice as part of ingest. Ran at approx. 3x speed on MPEG-2 broadcast files.
					</textarea>
				</section>
				<section>
                    <p>Each ninety frame 'Rough Signature' consisting of five 'BagOfWords' sections stored in database with corresponding in/out frame numbers.
					<pre><code data-trim>
					<BagOfWords>0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  1  0  0  0  0  1  0  0  0  0  0  0  0  0  0 </BagOfWords>
					</code></pre>
					<aside class="notes">
					Decided to store only five elements (bags of words) of rough fingerprint as frame level accuracy not needed. 90 frame segments with 45 frames of overlap was close enough for us!
					</aside>
				</section>

                <section data-markdown>
                    <textarea data-template>
                        ## Query Process
                        Generate fingerprint for input -> Parse fingerprint for 'rough fingerprints' -> Query database for 'rough fingerprints' (using exact matching) -> Find in/out frames for matching fingerprints -> Parse in/out into 500 frame segments -> Generate preview from input file using time stamps for matching hashes

                        Note:
                        Query using exact matching (rather than Hamming Distance) for simplicity, and because ballpark was good enough for purposes
                    </textarea>
                </section>

				<section>
				<pre><code data-trim data-noescape>searchfingerprint -i 70 -o 85 ~/Desktop/temple.mp4</code></pre>
				<img src="Resources/results.gif" alt="Eddie Vedder results">
				<div class=citation>
				<p>Sample footage copyright Temple of the Dog/A&amp;M Records</p>
				</div>
				<aside class="notes">
				Yeaaaaaaaarrhhhhhh

				Found both record for original file and transcoded ogg vorbis
				</aside>
				</section>

				 <section>
 					<p>'Brewery Guy Redux'</p>
					<img src="Resources/breweryguy.jpg" alt="Brewery Guy Results" width=60% height=60%>

					<div class="citation">
					<p>April - 1700 videos, 1.7 million fingerprints. November - 3177 videos, 3.4 million fingerprints. No appreciable difference in generation/search time of around 30 seconds.</p>
					</div>
					<aside class="notes">
					Recent query tested using transcoded lossy download from youtube.
					Almost all of that time is for generating the fingerprint - actual DB query in matter of seconds due to indexing on ingest.
					</aside>
				</section>

				<section data-background-iframe="https://amiaopensource.github.io/ffmprovisr/#generate_video_fingerprint" data-background-interactive>
					<div class="contrast">
						<pre><h3>https://amiaopensource.github.io/ffmprovisr</h3></pre>
					</div>
					<aside class="notes">
					ffmprovisr spearheaded by inimitable Ashley Blewer!
					</aside>
				</section>

				<section data-markdown>
					<textarea data-template>
					### Resources
					* [MPEG Representation of Digital Media](http://www.worldcat.org/oclc/902763394)
					* [How Does Chromaprint Work?](https://oxygene.sk/2011/01/how-does-chromaprint-work/)
					* [Adventures in Perceptual Hashing](https://privatezero.github.io/weaverblog/2017/04/18/Adventures-in-perceptual-hashing.html)
					* [ffmprovisr](https://amiaopensource.github.io/ffmprovisr)
					#### Tools
					* [AccoustID](https://acoustid.org/)
					* [pyacoustid](https://pypi.python.org/pypi/pyacoustid)
					* [FFmpeg](https://www.ffmpeg.org/)

					</textarea>
				</section>

			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>
